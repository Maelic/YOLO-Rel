# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license

"""
Relationship Detection Validation.
This module implements validation for relationship detection models.
"""

import json
import numpy as np
import torch
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any

from ultralytics.models.yolo.detect import DetectionValidator
from ultralytics.utils import LOGGER, ops
from ultralytics.utils.metrics import ConfusionMatrix, DetMetrics, box_iou, RelMetrics


class RelationValidator(DetectionValidator):
    """
    Validator for relationship detection models.
    
    This class extends DetectionValidator to handle relationship prediction
    validation alongside object detection.
    
    Attributes:
        relationship_metrics (dict): Metrics for relationship prediction
        relation_confusion_matrix (ConfusionMatrix): Confusion matrix for relationships
        relation_stats (list): Statistics for relationship predictions
        
    Examples:
        >>> validator = RelationValidator(args=args)
        >>> metrics = validator(model=model)
        >>> print(f"Relationship mAP@0.5: {metrics.relation_map50}")
    """
    
    def __init__(self, dataloader=None, save_dir=None, args=None, _callbacks=None):
        """
        Initialize RelationValidator.
        
        Args:
            dataloader (torch.utils.data.DataLoader, optional): Dataloader for validation dataset
            save_dir (Path, optional): Directory to save validation results
            args (SimpleNamespace, optional): Configuration arguments
            _callbacks (dict, optional): Dictionary of callback functions
        """
        super().__init__(dataloader, save_dir, args, _callbacks)
        
        # Relationship-specific attributes
        self.relation_nc = getattr(args, 'relation_nc', 56)  # Default to PSG relation classes
        self.relation_names = getattr(args, 'relation_names', {i: f'relation_{i}' for i in range(self.relation_nc)})
        
        # Use RelMetrics for comprehensive relation evaluation
        self.relation_metrics = None
        
        # Storage for relation predictions and ground truth
        self.relation_preds = []  # [(subj_idx, obj_idx, rel_cls, conf), ...]
        self.relation_gts = []    # [(subj_idx, obj_idx, rel_cls), ...]
        
        LOGGER.info(f"RelationValidator initialized with {self.relation_nc} relation classes")

    def init_metrics(self, model):
        """Initialize metrics for both detection and relationship validation."""
        super().init_metrics(model)
        
        # Initialize relationship-specific metrics using RelMetrics
        self.relation_metrics = RelMetrics(save_dir=self.save_dir, plot=self.args.plots, names=self.relation_names)
        
    def preprocess(self, batch):
        """Preprocess batch for validation including relationship data."""
        batch = super().preprocess(batch)
        
        # Handle relationship data if present
        if 'relations' in batch:
            batch['relations'] = [rel for rel in batch['relations']]
        if 'relation_labels' in batch:
            batch['relation_labels'] = batch['relation_labels'].to(self.device, non_blocking=True)
        if 'object_pairs' in batch:
            batch['object_pairs'] = [pairs for pairs in batch['object_pairs']]
        if 'num_relations' in batch:
            batch['num_relations'] = batch['num_relations'].to(self.device, non_blocking=True)
            
        return batch

    def postprocess(self, preds):
        """Apply Non-maximum suppression to prediction outputs."""        
        # Handle both detection and relationship predictions
        if isinstance(preds, (list, tuple)) and len(preds) == 2:
            detection_preds, relation_preds = preds
            
            # Apply NMS to detection predictions
            nms_outputs = ops.non_max_suppression(
                detection_preds,
                self.args.conf,
                self.args.iou,
                nc=0 if self.args.task == "detect" else self.nc,
                multi_label=True,
                agnostic=self.args.single_cls or self.args.agnostic_nms,
                max_det=self.args.max_det,
                end2end=getattr(self, 'end2end', False),
                rotated=self.args.task == "obb",
            )
            
            # Format detection predictions as expected by update_metrics
            formatted_detections = [{"bboxes": x[:, :4], "conf": x[:, 4], "cls": x[:, 5], "extra": x[:, 6:]} for x in nms_outputs]
            
            # Store relation predictions for later use
            self._relation_preds = relation_preds
            
            return formatted_detections
        else:
            # Standard detection only
            return super().postprocess(preds)

    def update_metrics(self, preds, batch):
        """Update metrics for both detection and relationships."""
        # Standard detection metrics update
        super().update_metrics(preds, batch)
        
        # Update relationship metrics if relation predictions are available
        if hasattr(self, '_relation_preds') and self._relation_preds is not None:
            self._update_relationship_metrics(preds, self._relation_preds, batch)
            # Clear stored relation predictions
            self._relation_preds = None

    def _update_relationship_metrics(self, detection_preds, relation_preds, batch):
        """Update relationship-specific metrics."""        
        # Check if relation_preds is actually empty or invalid
        if relation_preds is None:
            return
        elif isinstance(relation_preds, (tuple, list)):
            if len(relation_preds) == 0:
                return
            elif isinstance(relation_preds, tuple) and len(relation_preds) == 2:
                # Handle tuple from Detect head (auxiliary outputs, main predictions)
                # Use the actual relation predictions (should be a tensor)
                relation_logits = None
                for item in relation_preds:
                    if isinstance(item, torch.Tensor) and item.dim() >= 2:
                        relation_logits = item
                        break
                        
                if relation_logits is None:
                    return
                    
                relation_preds = relation_logits
        
        if not hasattr(batch, 'relations') and 'relations' not in batch:
            return
        
        # Handle relation predictions as tensor
        if isinstance(relation_preds, torch.Tensor):
            # relation_preds shape should be [batch_size, num_relation_classes, num_anchors]
            batch_size = relation_preds.shape[0]
            
            for si in range(batch_size):
                # Get ground truth relationships for this sample
                if 'relations' in batch and si < len(batch['relations']):
                    target_relations = batch['relations'][si]
                else:
                    target_relations = np.array([])
                    
                if len(target_relations) == 0:
                    continue
                
                # Convert to tensor if needed and get object pairs from ground truth relations
                if isinstance(target_relations, (list, np.ndarray)):
                    target_relations = torch.tensor(target_relations)
                
                # target_relations format: [subj_idx, obj_idx, rel_cls]
                gt_pairs = target_relations[:, :2].int()  # Object pairs
                gt_rel_cls = target_relations[:, 2].int()  # Relation classes
                
                # Process relationship predictions for this sample
                sample_rel_preds = relation_preds[si]  # Shape: [num_relation_classes, num_anchors]
                
                # Convert logits to predictions using sigmoid (for multi-label) instead of softmax
                rel_scores = torch.sigmoid(sample_rel_preds)  # Shape: [num_relation_classes, num_anchors]
                
                # Get the best relation class for each anchor
                max_scores, pred_rel_cls = torch.max(rel_scores, dim=0)  # Get best relation per anchor
                
                # Use much lower confidence threshold since sigmoid scores can be lower
                conf_threshold = 0.01  # Much lower threshold
                valid_indices = max_scores > conf_threshold
                
                if torch.sum(valid_indices) == 0:
                    continue
                
                # For simplicity, assume anchor indices map to object pair indices
                # In practice, you'd need proper anchor-to-object mapping
                valid_scores = max_scores[valid_indices]
                valid_classes = pred_rel_cls[valid_indices]
                valid_anchor_indices = torch.where(valid_indices)[0]
                
                # Create pseudo object pairs (this is simplified - need proper implementation)
                # For now, just create some sample pairs for testing
                batch_pred_relations = []
                for i, (anchor_idx, rel_class, confidence) in enumerate(zip(valid_anchor_indices, valid_classes, valid_scores)):
                    if i >= len(gt_pairs):  # Don't exceed ground truth for this test
                        break
                    # Use ground truth pairs for now (in practice, derive from detections)
                    subj_idx, obj_idx = gt_pairs[i].tolist()
                    batch_pred_relations.append((subj_idx, obj_idx, rel_class.item(), confidence.item()))
                
                # Convert ground truth to format: [(subj_idx, obj_idx, rel_cls), ...]
                batch_gt_relations = []
                for i in range(len(target_relations)):
                    subj_idx, obj_idx = gt_pairs[i].tolist()
                    rel_class = gt_rel_cls[i].item()
                    batch_gt_relations.append((subj_idx, obj_idx, rel_class))
                
                # Store for final processing
                self.relation_preds.extend(batch_pred_relations)
                self.relation_gts.extend(batch_gt_relations)
        else:
            return

    def finalize_metrics(self, *args, **kwargs):
        """Finalize and compute all metrics."""
        # Finalize detection metrics first
        super().finalize_metrics(*args, **kwargs)
        
        # Finalize relationship metrics if we have relation data
        if self.relation_preds and self.relation_gts:
            # Process relation stats with RelMetrics
            self.relation_metrics.process_relation_stats(
                pred_relations=self.relation_preds,
                gt_relations=self.relation_gts,
                relation_names=self.relation_names
            )
            
            # Update main metrics with relationship results
            relation_results = self.relation_metrics.mean_results()
            
            # Extract specific metrics (detection + relation)
            if len(relation_results) >= 10:  # 4 detection + 6 relation metrics
                self.metrics.relation_recall_20 = relation_results[4]   # Recall@20
                self.metrics.relation_mean_recall_20 = relation_results[5]  # MeanRecall@20
                self.metrics.relation_recall_50 = relation_results[6]   # Recall@50
                self.metrics.relation_mean_recall_50 = relation_results[7]  # MeanRecall@50
                self.metrics.relation_recall_100 = relation_results[8]  # Recall@100
                self.metrics.relation_mean_recall_100 = relation_results[9] # MeanRecall@100
            
            LOGGER.info(f"Relationship Validation Results:")
            LOGGER.info(f"  Total predicted relations: {len(self.relation_preds)}")
            LOGGER.info(f"  Total ground truth relations: {len(self.relation_gts)}")
            LOGGER.info(f"  Recall@20: {getattr(self.metrics, 'relation_recall_20', 0.0):.4f}")
            LOGGER.info(f"  MeanRecall@20: {getattr(self.metrics, 'relation_mean_recall_20', 0.0):.4f}")
            LOGGER.info(f"  Recall@50: {getattr(self.metrics, 'relation_recall_50', 0.0):.4f}")
            LOGGER.info(f"  MeanRecall@50: {getattr(self.metrics, 'relation_mean_recall_50', 0.0):.4f}")
            LOGGER.info(f"  Recall@100: {getattr(self.metrics, 'relation_recall_100', 0.0):.4f}")
            LOGGER.info(f"  MeanRecall@100: {getattr(self.metrics, 'relation_mean_recall_100', 0.0):.4f}")
        else:
            LOGGER.info("No relationship predictions or ground truth available for evaluation")
            LOGGER.info(f"  Recall@20: {getattr(self.metrics, 'relation_recall_20', 0.0):.4f}")
            LOGGER.info(f"  MeanRecall@20: {getattr(self.metrics, 'relation_mean_recall_20', 0.0):.4f}")
            LOGGER.info(f"  Recall@50: {getattr(self.metrics, 'relation_recall_50', 0.0):.4f}")
            LOGGER.info(f"  MeanRecall@50: {getattr(self.metrics, 'relation_mean_recall_50', 0.0):.4f}")
            LOGGER.info(f"  Recall@100: {getattr(self.metrics, 'relation_recall_100', 0.0):.4f}")
            LOGGER.info(f"  MeanRecall@100: {getattr(self.metrics, 'relation_mean_recall_100', 0.0):.4f}")

    def get_desc(self) -> str:
        """Return a formatted string summarizing class metrics of YOLO relation model."""
        # Standard detection format plus mR@100: Class, Images, Instances, Box(P, R, mAP50, mAP50-95), mR@100
        return ("%22s" + "%11s" * 7) % ("Class", "Images", "Instances", "Box(P", "R", "mAP50", "mAP50-95)", "mR@100")

    def get_stats(self) -> Dict[str, Any]:
        """Calculate and return metrics statistics. Keep compatible with detection CSV format."""
        # Get detection stats only to maintain CSV compatibility
        stats = super().get_stats()
        
        # Note: Relationship metrics are tracked separately to avoid CSV format issues
        # Access via: self.metrics.relation_mean_recall_100, etc.
        
        return stats

    def print_results(self) -> None:
        """Print training/validation set metrics per class including relationships."""
        # Print detection results in standard YOLO format with mR@100 added
        mr100 = getattr(self.metrics, 'relation_mean_recall_100', 0.0) if hasattr(self.metrics, 'relation_mean_recall_100') else 0.0
        
        # Standard detection format with added mR@100
        pf = "%22s" + "%11i" * 2 + "%11.3g" * len(self.metrics.keys) + "%11.3g"
        LOGGER.info(pf % ("all", self.seen, self.metrics.nt_per_class.sum(), *self.metrics.mean_results(), mr100))
        
        if self.metrics.nt_per_class.sum() == 0:
            LOGGER.warning(f"no labels found in {self.args.task} set, can not compute metrics without labels")

        # Print results per class (detection + mR@100)
        if self.args.verbose and not self.training and self.nc > 1 and len(self.metrics.stats):
            for i, c in enumerate(self.metrics.ap_class_index):
                LOGGER.info(
                    pf % (
                        self.names[c],
                        self.metrics.nt_per_image[c],
                        self.metrics.nt_per_class[c],
                        *self.metrics.class_result(i),  # All detection metrics for this class
                        mr100  # Overall mR@100 (same for all classes)
                    )
                )

        # Print enhanced relationship results table
        if hasattr(self.metrics, 'relation_recall_20'):
            self._print_relationship_table()
        else:
            LOGGER.info("\n" + "="*70)
            LOGGER.info("RELATIONSHIP METRICS")
            LOGGER.info("="*70)
            LOGGER.info("No relationship predictions or ground truth available for evaluation")
            LOGGER.info("="*70)

    def _print_relationship_table(self):
        """Print a nicely formatted relationship metrics table."""
        LOGGER.info("\n" + "="*70)
        LOGGER.info("RELATIONSHIP DETECTION RESULTS")
        LOGGER.info("="*70)
        
        # Summary information
        total_preds = len(self.relation_preds) if hasattr(self, 'relation_preds') else 0
        total_gts = len(self.relation_gts) if hasattr(self, 'relation_gts') else 0
        
        LOGGER.info(f"Dataset: {total_preds:,} predictions, {total_gts:,} ground truth relations")
        LOGGER.info(f"Classes: {self.relation_nc} relation types")
        
        # Check if detection metrics are meaningful
        detection_metrics = self.metrics.mean_results()
        if all(metric == 0.0 for metric in detection_metrics):
            LOGGER.info("Note: Detection mAP=0.0 indicates model needs training for meaningful object detection")
        
        LOGGER.info("-" * 70)
        
        # Metrics table header
        LOGGER.info(f"{'Metric':<25} {'@20':<12} {'@50':<12} {'@100':<12}")
        LOGGER.info("-" * 70)
        
        # Get metrics values
        recall_20 = getattr(self.metrics, 'relation_recall_20', 0.0)
        recall_50 = getattr(self.metrics, 'relation_recall_50', 0.0)
        recall_100 = getattr(self.metrics, 'relation_recall_100', 0.0)
        
        mean_recall_20 = getattr(self.metrics, 'relation_mean_recall_20', 0.0)
        mean_recall_50 = getattr(self.metrics, 'relation_mean_recall_50', 0.0)
        mean_recall_100 = getattr(self.metrics, 'relation_mean_recall_100', 0.0)
        
        # Print metrics rows
        LOGGER.info(f"{'Recall':<25} {recall_20:<12.4f} {recall_50:<12.4f} {recall_100:<12.4f}")
        LOGGER.info(f"{'Mean Recall':<25} {mean_recall_20:<12.4f} {mean_recall_50:<12.4f} {mean_recall_100:<12.4f}")
        
        LOGGER.info("-" * 70)
        
        # Overall performance summary
        if hasattr(self.relation_metrics, 'fitness'):
            LOGGER.info(f"Combined Fitness Score: {self.relation_metrics.fitness:.4f}")
        
        # Performance interpretation
        if mean_recall_100 > 0.1:
            performance = "Good"
        elif mean_recall_100 > 0.05:
            performance = "Fair"
        elif mean_recall_100 > 0.01:
            performance = "Poor"
        else:
            performance = "Very Poor"
            
        LOGGER.info(f"Overall Relation Performance: {performance} (mR@100: {mean_recall_100:.4f})")
        
        if mean_recall_100 < 0.01:
            LOGGER.info("ðŸ’¡ Tip: Low relation performance expected for untrained models")
            
        LOGGER.info("="*70)

    def save_json(self, stats, save_dir=Path("."), file_name="relation_results.json"):
        """Save relationship validation results to JSON."""
        # Save detection results
        super().save_json(stats)
        
        # Save relationship results
        if hasattr(self.metrics, 'relation_recall_20'):
            relation_results = {
                'recall_at_20': float(getattr(self.metrics, 'relation_recall_20', 0.0)),
                'mean_recall_at_20': float(getattr(self.metrics, 'relation_mean_recall_20', 0.0)),
                'recall_at_50': float(getattr(self.metrics, 'relation_recall_50', 0.0)),
                'mean_recall_at_50': float(getattr(self.metrics, 'relation_mean_recall_50', 0.0)),
                'recall_at_100': float(getattr(self.metrics, 'relation_recall_100', 0.0)),
                'mean_recall_at_100': float(getattr(self.metrics, 'relation_mean_recall_100', 0.0)),
                'relation_classes': self.relation_names,
                'validation_summary': {
                    'total_images': len(self.dataloader.dataset) if self.dataloader else 0,
                    'relation_classes': self.relation_nc,
                    'total_predictions': len(self.relation_preds),
                    'total_ground_truth': len(self.relation_gts),
                }
            }
            
            # Add fitness score if available
            if hasattr(self.relation_metrics, 'fitness'):
                relation_results['fitness'] = float(self.relation_metrics.fitness)
            
            # Save to file
            save_path = save_dir / file_name
            with open(save_path, 'w') as f:
                json.dump(relation_results, f, indent=2)
            
            LOGGER.info(f"Relationship validation results saved to {save_path}")
        else:
            LOGGER.info("No relationship metrics to save")

    def build_dataset(self, img_path: str, mode: str = "val", batch: Optional[int] = None):
        """Build relationship dataset for validation."""
        # Import here to avoid circular imports
        from ultralytics.data.build import build_yolo_relation_dataset
        
        # Get relation file from data config
        relation_file = None
        if hasattr(self, 'data') and self.data:
            relation_key = f"relation_{mode}"
            if relation_key in self.data:
                relation_file = self.data[relation_key]
        
        if relation_file:
            # Make relation file path absolute if it's relative
            import os
            from pathlib import Path
            
            if not os.path.isabs(relation_file) and hasattr(self, 'data') and 'path' in self.data:
                relation_file = str(Path(self.data['path']) / relation_file)
            
            LOGGER.info(f"Building relation dataset for validation with relation file: {relation_file}")
            
            # Build relationship dataset
            gs = 32  # Default stride - we'll get the actual stride from model if available
            if hasattr(self, 'model') and self.model and hasattr(self.model, 'stride'):
                gs = max(int(self.model.stride.max()), 32)
            
            # Ensure the task is set to 'relation' for the dataset
            original_task = getattr(self.args, 'task', None)
            self.args.task = 'relation'  # Temporarily set task to relation
            
            try:
                dataset = build_yolo_relation_dataset(
                    self.args,
                    img_path=img_path,
                    relation_file=relation_file,
                    mode=mode,
                    stride=gs,
                    batch=batch or self.args.batch,
                    data=self.data,
                )
            finally:
                # Restore original task
                if original_task is not None:
                    self.args.task = original_task
                    
            return dataset
        else:
            LOGGER.warning(f"No relation file found for {mode} mode, using standard detection dataset")
            return super().build_dataset(img_path, mode, batch)
